{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e9e35c8b347c6a8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:49:34.648594Z",
     "start_time": "2025-07-30T08:49:26.257320Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer,AutoModelForCausalLM",
   "id": "e078cb39fde187eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/profniggastein/PycharmProjects/ReducedDataset/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T09:02:24.426002Z",
     "start_time": "2025-07-30T09:02:23.669482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#use deepseek r1 nodel\n",
    "model_id = \"deepseek-ai/DeepSeek-R1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,trust_remote_code=True)"
   ],
   "id": "52dbbd3527c9fd6a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m model_id = \u001B[33m\"\u001B[39m\u001B[33mdeepseek-ai/DeepSeek-R1\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      3\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ReducedDataset/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:550\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    548\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_remote_code \u001B[38;5;129;01mand\u001B[39;00m trust_remote_code:\n\u001B[32m    549\u001B[39m     class_ref = config.auto_map[\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m550\u001B[39m     model_class = \u001B[43mget_class_from_dynamic_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclass_ref\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    552\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    553\u001B[39m     _ = hub_kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mcode_revision\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    554\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m os.path.isdir(pretrained_model_name_or_path):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ReducedDataset/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:489\u001B[39m, in \u001B[36mget_class_from_dynamic_module\u001B[39m\u001B[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001B[39m\n\u001B[32m    487\u001B[39m     code_revision = revision\n\u001B[32m    488\u001B[39m \u001B[38;5;66;03m# And lastly we get the class inside our newly created module\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m489\u001B[39m final_module = \u001B[43mget_cached_module_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodule_file\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m.py\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    500\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    501\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m get_class_in_module(class_name, final_module)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ReducedDataset/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:315\u001B[39m, in \u001B[36mget_cached_module_file\u001B[39m\u001B[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    312\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m    314\u001B[39m \u001B[38;5;66;03m# Check we have all the requirements in our environment\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m315\u001B[39m modules_needed = \u001B[43mcheck_imports\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresolved_module_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m \u001B[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001B[39;00m\n\u001B[32m    318\u001B[39m full_submodule = TRANSFORMERS_DYNAMIC_MODULE_NAME + os.path.sep + submodule\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ReducedDataset/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:180\u001B[39m, in \u001B[36mcheck_imports\u001B[39m\u001B[34m(filename)\u001B[39m\n\u001B[32m    177\u001B[39m         missing_packages.append(imp)\n\u001B[32m    179\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_packages) > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m    181\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThis modeling file requires the following packages that were not found in your environment: \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    182\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(missing_packages)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. Run `pip install \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m.join(missing_packages)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    183\u001B[39m     )\n\u001B[32m    185\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m get_relative_imports(filename)\n",
      "\u001B[31mImportError\u001B[39m: This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer.save_pretrained(\"./deepseek-chat-r1/\")\n",
    "model.save_pretrained(\"./deepseek-chat-r1/\")"
   ],
   "id": "5ca3568a00e16152"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
